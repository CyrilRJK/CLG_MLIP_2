{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import time\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "    for store_id in STORES_IDS:\n",
    "        #temp_df = pd.read_pickle('../input/m5-aux-models/test_'+store_id+'.pkl')\n",
    "        temp_df = pd.read_pickle('../input/clg2aux/test_'+str(store_id)+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "        \n",
    "    return base_test\n",
    "\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_lag_roll(LAG_DAY,grid):\n",
    "    for lag in LAG_DAY:\n",
    "        shift_day = lag[0]\n",
    "        roll_wind = lag[1]\n",
    "        gr = grid.groupby([\"id\"])[\"demand\"]\n",
    "        #lag_df = base_test[['id','d',TARGET]]\n",
    "        col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "        grid[col_name] = gr.transform(lambda x: x.shift(shift_day).rolling(roll_wind,min_periods=1).mean())\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "#lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'demand'           # Our target (was sales)\n",
    "START_TRAIN = 0             # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "#AUX_MODELS = '../input/m5-aux-models/'\n",
    "\n",
    "# These features are the ones that lead to overfit and the ones that \n",
    "# are not in the test set \n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d','wday', 'year', 'month',TARGET]\n",
    "\n",
    "#STORES ids\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "#STORES_IDS =  pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "#STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORES_IDS = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMANDD FEATURES IN PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAGS = [7, 28]\n",
    "WINDOWS = [7, 14, 28]\n",
    "\n",
    "def demand_features(df):\n",
    "    \"\"\" Derive features from sales data and remove rows with missing values \"\"\"\n",
    "    \n",
    "    for lag in LAGS:\n",
    "        df[f'lag_t{lag}'] = df.groupby('id')['demand'].transform(lambda x: x.shift(lag)).astype(\"float32\")\n",
    "        for w in WINDOWS:\n",
    "            df[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id')[f'lag_t{lag}'].transform(lambda x: x.rolling(w).mean()).astype(\"float32\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def demand_features_eval(df):\n",
    "    \"\"\" Same as demand_features but for the step-by-step evaluation \"\"\"\n",
    "    out = df.groupby('id', sort=False).last()\n",
    "    for lag in LAGS:\n",
    "        out[f'lag_t{lag}'] = df.groupby('id', sort=False)['demand'].nth(-lag-1).astype(\"float32\")\n",
    "        for w in WINDOWS:\n",
    "            out[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id', sort=False)['demand'].nth(list(range(-lag-w, -lag))).groupby('id', sort=False).mean().astype(\"float32\")\n",
    "    return out.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')\n",
    "#submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "#submission.to_csv('submission_v'+str(VER)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST = 1914\n",
    "LENGTH = 28\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "MODEL_FEATURES = [col for col in list(base_test) if col not in remove_features]\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "\n",
    "#for PREDICT_DAY in range(1,29):   \n",
    "\n",
    "for i, day in enumerate(np.arange(FIRST, FIRST + LENGTH)):\n",
    "    print('Predict | Day:', day)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    #grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll,ROLS_SPLIT),df_parallelize_run(make_lag,ROLS_SPLIT)], axis=1)\n",
    "    #\n",
    "    \n",
    "    grid_df = base_test.copy()\n",
    "    \n",
    "    test_day = demand_features_eval(grid_df[(grid_df.d <= day) & (grid_df.d >= day - max(LAGS) - max(WINDOWS))])\n",
    "    for store_id in STORES_IDS:\n",
    "    \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = '../input/clg2aux/lgb_model_'+str(store_id)+'_v'+str(VER)+'.bin' \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==day\n",
    "    \n",
    "        \n",
    "        base_test.loc[((base_test.d == day) & (base_test.store_id == store_id)),\"demand\"]= estimator.predict(test_day[test_day.store_id==store_id][MODEL_FEATURES])\n",
    "       \n",
    "                \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[base_test.d==day][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(i+1)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(i+1)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.to_csv('predictions.csv', index=False)\n",
    "all_preds = all_preds.assign(id=all_preds.id+\"_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(1)\n",
    "submission.to_csv('submission_v1'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
