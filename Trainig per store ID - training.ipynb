{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.kaggle.com/mayer79/m5-forecast-keras-with-categorical-embeddings-v2 \n",
    "\n",
    "[2] https://www.kaggle.com/ragnar123/very-fst-model\n",
    "\n",
    "[3] https://www.kaggle.com/mayer79/m5-forecast-poisson-loss\n",
    "\n",
    "[4] https://www.kaggle.com/lnovakovi/copy-m5-darker-magic-difflgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import time\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Memory Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function on dataframes to reduce the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "'''\n",
    "path = \"../input/m5-forecasting-accuracy\"\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\n",
    "sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions from [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get data by store - the function in which we upload our dataframe with features \n",
    "* Get base test - the function that returns preprocessed test set-  it has all the features that author of [4] created (rolling means 7 ... 60 I think, lag features, mean encoding etc). We can always drop the features we don't want to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic features here, just related to e.g. calendar and\n",
    "    # prices \n",
    "    \n",
    "    #The guy had 4 pickle files of features, first load simple ones\n",
    "    \n",
    "    #Here we can load our features what we have\n",
    "    '''df = pd.concat([pd.read_pickle(ID),\n",
    "                    pd.read_pickle(INITIAL).iloc[:,2:],\n",
    "                    pd.read_pickle(PART2).iloc[:,2:]],\n",
    "                   \n",
    "                    axis=1)\n",
    "    '''\n",
    "    # In our case we have sales as melted dataframe, and we leave relevant part \n",
    "    \n",
    "    df=sales\n",
    "    df = df[df['store_id']==store]\n",
    "    \n",
    "    \n",
    "    '''With memory limits we have to read \n",
    "    lags and mean encoding features\n",
    "    separately and drop items that we don't need.\n",
    "    As our Features Grids are aligned \n",
    "    we can use index to keep only necessary rows\n",
    "    Alignment is good for us as concat uses less memory than merge. '''\n",
    "    \n",
    "    # We add lag and rolling features seperately and leave only one related\n",
    "    # to indexes we have, so only ones for current STORE ID \n",
    "    \n",
    "    '''\n",
    "    df2 = pd.read_pickle(ROLLING)\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit '''\n",
    "    \n",
    "    # Note: Here we can also add functions to dynamically make some new features \n",
    "    \n",
    "    # Create features list \n",
    "    # note: feature list is defined in the cell with the rest of the global variables\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    #print(df)\n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "    for store_id in STORES_IDS:\n",
    "        #temp_df = pd.read_pickle('../input/m5-aux-models/test_'+store_id+'.pkl')\n",
    "        temp_df = pd.read_pickle('test_'+str(store_id)+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "        \n",
    "    return base_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help functions from [4] for making features dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes dynamically features (used in the prediction part I think\n",
    "# but if we don't have pickle files we can use them in training I think as well)\n",
    "\n",
    "def make_lag(LAG_DAY):\n",
    "    #TARGET=\"sales\"\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    #TARGET=\"sales\"\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model LGBM+Fourier+Features3(Cyril'sBest)+Categorical\n",
    "'''\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'metric': 'rmse',\n",
    "                    'objective': 'poisson',\n",
    "                    'seed': 200,\n",
    "                    'force_row_wise' : True,\n",
    "                    'learning_rate' : 0.01,\n",
    "                    'lambda': 0.1,\n",
    "                    'num_leaves': 200,\n",
    "                    'sub_row' : 0.7,\n",
    "                    'bagging_freq' : 1,\n",
    "                    'colsample_bytree': 0.77\n",
    "    }\n",
    "\n",
    "'''\n",
    "\n",
    "# taken from [4]\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.031,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file to store encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file = open('encoders', 'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything the same as we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- The columns \"Date\" and \"Weekday\" are dropped as they contain redundant information.\n",
    "- Normally, the column \"d\" is like \"d_1,d_2,...\". Make it \"1,2,..\" and the type integer\n",
    "- If there is no event (I think), there is NA. We will replace them with \"NoEvent\" string. Originally, it was replaced with\n",
    "  \"missing\", but I don't think it makes sense as I don't think there is missing information, I think they just left \n",
    "  the days without any event as NA.\n",
    "- We enumerate most of the columns:\n",
    "    - We do not enumerate \"d\" and \"wm_yr_weak\" because we will use these columns for joins.\n",
    "    - Why do we enumerate month and day? I think it is because they start from 1, not 0.\n",
    "    - Originally, the binary columns \"snap_X\" were also enumerated. I don't think it is necessary. The only neccessary step\n",
    "      was to convert their type from int64 to int as it uses less space; but reduce_mem_usage will take care of that.\n",
    "- I would suggest saving the OrdinalEncoder in case we need to reverse the transformations\n",
    "'''\n",
    "#def prep_calendar(df,encoder_file):\n",
    "#    df = df.drop([\"date\", \"weekday\"], axis=1)  \n",
    "#    df = df.assign(d = df.d.str[2:].astype(int))\n",
    "#    df = df.fillna(\"NoEvent\")\n",
    "#    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"}) \n",
    "#    oe = OrdinalEncoder(dtype=\"int\")\n",
    "#    df[cols] = oe.fit_transform(df[cols])\n",
    "#    pickle.dump(oe,encoder_file)\n",
    "#    df = reduce_mem_usage(df)\n",
    "#    return df\n",
    "#\n",
    "#calendar = prep_calendar(calendar,encoder_file)\n",
    "\n",
    "'''\n",
    "Got from: [1]\n",
    "Originally, there were features added in this part. I excluded them until we decide whether to use those or not.\n",
    "'''\n",
    "#def prep_selling_prices(df):\n",
    "#    df = reduce_mem_usage(df)\n",
    "#    return df\n",
    "#\n",
    "#selling_prices = prep_selling_prices(selling_prices)\n",
    "\n",
    "\n",
    "'''\n",
    "Got from: [1]\n",
    "- We drop the first \"drop_d\" days. Originally, this is set to 1000. When it is set to this value,\n",
    "  the shape we get 29,544,810 rows. When we don't set it, we get 60,034,810 rows. I think for now \n",
    "  we can keep this functionality, as it may be useful if we would like to discard some of the days.\n",
    "- In some id's, we have \"_validation\". Those are deleted.\n",
    "- reindex: Conform DataFrame to new index with optional filling logic (obtained from pandas doc). \n",
    "  We add days 1914+2*28 to prepare data from submission\n",
    "- We have to melt the sales dataframe since days are contained as columns.\n",
    "- assign: Returns a new object with all original columns in addition to new ones. Existing columns \n",
    "  that are re-assigned will be overwritten (obtained from pandas doc). Again, we make the values \n",
    "  \"d_1, d-2,...\" to \"1,2,...\"\n",
    "'''\n",
    "##We have to melt sales for sure because the days are columns, which is not desirable.\n",
    "#def reshape_sales(df, drop_d = None):\n",
    "#    if drop_d is not None:\n",
    "#        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n",
    "#    #df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n",
    "#    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n",
    "#    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "#                 var_name='d', value_name='demand')\n",
    "#    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n",
    "#    return df\n",
    "#\n",
    "#sales = reshape_sales(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- Merge all the dataframes and delete the unnecessary ones\n",
    "- time.sleep() added to make sure garbage collector finishes its job before the next merge\n",
    "'''\n",
    "#sales = sales.merge(calendar, how=\"left\", on=\"d\")\n",
    "#del calendar\n",
    "#gc.collect()\n",
    "#time.sleep(5)\n",
    "#sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "#del selling_prices\n",
    "#sales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "#gc.collect()\n",
    "#time.sleep(5)\n",
    "#sales.head()\n",
    "#\n",
    "#sales = reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add features in the begining as pickle files. The idea is from [4]. This helps also solve memory issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "                    'clusters', 'd', 'demand', 'wday', 'month', 'year', 'event_name_1',\n",
    "                    'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n",
    "                    'snap_WI', 'daytype', 'sell_price', 'sell_price_rel_diff']\n",
    "\n",
    "rolling_windows = ['lag_t7','rolling_mean_lag7_w7', 'rolling_mean_lag7_w14',\n",
    "                   'rolling_mean_lag7_w28', 'rolling_mean_lag7_w60',\n",
    "                   'rolling_mean_lag7_w90', 'lag_t28', 'rolling_mean_lag28_w7',\n",
    "                   'rolling_mean_lag28_w14', 'rolling_mean_lag28_w28',\n",
    "                   'rolling_mean_lag28_w60', 'rolling_mean_lag28_w90']\n",
    "\n",
    "simple_fe_features = ['price_max', 'price_min',\n",
    "                      'price_std', 'price_mean', 'price_norm', 'price_nunique',\n",
    "                      'item_nunique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.DataFrame()\n",
    "\n",
    "ids = []\n",
    "with (open('../input/clgfeatures/id column', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            ids.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "            \n",
    "ids = [item for sublist in ids for item in sublist]\n",
    "sales[\"id\"]=ids         \n",
    "del ids\n",
    "gc.collect()\n",
    "print(\"added ids\")\n",
    "\n",
    "init_f = []\n",
    "with (open('../input/clgfeatures/initial features', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            init_f.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "            \n",
    "for idx,feat in enumerate(initial_features):\n",
    "    sales[feat]=init_f[idx]\n",
    "    \n",
    "del init_f, initial_features\n",
    "sales= reduce_mem_usage(sales)\n",
    "gc.collect()\n",
    "   \n",
    "print(\"added initial features\" )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = []\n",
    "with (open('../input/clgfeatures/SimpleFE1', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            part.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "            \n",
    "for idx,p in enumerate(simple_fe_features):\n",
    "    sales[p]=part[idx]\n",
    "    \n",
    "del part, simple_fe_features\n",
    "sales= reduce_mem_usage(sales)\n",
    "gc.collect()\n",
    "\n",
    "print(\"added simple fe\")\n",
    "\n",
    "roll = []\n",
    "with (open('../input/clgfeatures/rolling windows', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            roll.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "            \n",
    "for idx,rol in enumerate(rolling_windows):\n",
    "    sales[rol]=roll[idx]\n",
    "    \n",
    "del roll, rolling_windows\n",
    "sales= reduce_mem_usage(sales)\n",
    "gc.collect()\n",
    "print(\"add roll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_fe_features2 = ['price_momentum', 'price_momentum_m','price_momentum_y']\n",
    "f2= []\n",
    "with (open('../input/clgfeatures/SimpleFE2', \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            f2.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "            \n",
    "for idx,fe in enumerate(simple_fe_features2):\n",
    "    sales[fe]=f2[idx]\n",
    "    \n",
    "del f2, simple_fe_features2\n",
    "sales= reduce_mem_usage(sales)\n",
    "gc.collect()\n",
    "print(\"add simple f2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Trying to add sales features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        ]\n",
    "TARGET='demand'\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    sales['enc'+col_name+'mean'] = sales.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    sales['enc'+col_name+'std'] = sales.groupby(col)[TARGET].transform('std').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is demand, but in his features the target was sales\n",
    "\n",
    "Taken from [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if we want to use mean encoding features, then we need to copy mean_features list from the Dark Magic notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#       defining some global variables , [4]\n",
    "\n",
    "\n",
    "\n",
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'demand'           # Our target (was sales)\n",
    "START_TRAIN = 0             # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "#AUX_MODELS = '../input/m5-aux-models/'\n",
    "\n",
    "# These features are the ones that lead to overfit and the ones that \n",
    "# are not in the test set \n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d','wday', 'year', 'month',TARGET]\n",
    "\n",
    "#STORES ids\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "#STORES_IDS =  pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "#STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "ID = '../input/clgfeatures/id column'\n",
    "INITIAL ='../input/clgfeatures/initial features'\n",
    "PART2 ='../input/clgfeatures/part2'\n",
    "ROLLING = '../input/clgfeatures/rolling windows'\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORES_IDS = list(sales['store_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    '''#make features for that store_id data\n",
    "    grid_df, features_columns = make_rolling_fe(grid_df) \n",
    "    grid_df= impute_Na(grid_df)\n",
    "    '''\n",
    "    print(\"made grid\")\n",
    "    \n",
    "\n",
    "    # train_mask (All data less than 1913)\n",
    "    # valid_mask (Last 28 days - not real validatio set)\n",
    "    # preds_mask (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features (100))\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "   \n",
    "    train_data.save_binary('train_data.bin')\n",
    "    train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    # Saving the part into the picke file \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+str(store_id)+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "\n",
    "    lgb.plot_importance(estimator, importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10));\n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    \n",
    "    # Why to use pickle file? \n",
    "    \n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+str(store_id)+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## taken from [4]\n",
    "'''\n",
    "ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission_v'+str(VER)+'.csv', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
