{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import OrdinalEncoder\nimport time\nfrom scipy import stats\nimport os\nimport pickle","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/m5-forecasting-accuracy\"\ncalendar = reduce_mem_usage(pd.read_csv(os.path.join(path, \"calendar.csv\")))\nselling_prices = reduce_mem_usage(pd.read_csv(os.path.join(path, \"sell_prices.csv\")))\nsales = reduce_mem_usage(pd.read_csv(os.path.join(path, \"sales_train_validation.csv\")))","execution_count":null,"outputs":[{"output_type":"stream","text":"Mem. usage decreased to  0.12 Mb (41.9% reduction)\nMem. usage decreased to 130.48 Mb (37.5% reduction)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)\nsns.set(style=\"whitegrid\")\ne1 = sns.countplot(calendar['event_name_1'], order= calendar['event_name_1'].value_counts().index, label='event1', ax=ax[0])\ne2 = sns.countplot(calendar['event_name_2'], order= calendar['event_name_2'].value_counts().index, label='event2', ax=ax[1])\ne1.set_xticklabels(e1.get_xticklabels(), rotation=90)\ne1.set_title('event_name_1')\ne2.set_xticklabels(e2.get_xticklabels(), rotation=45)\ne2.set_title('event_name_2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2, figsize=(12,8), sharex=False)\nsns.set(style=\"whitegrid\")\nd1 = sns.countplot(sales['state_id'], order= sales['state_id'].value_counts().index, label='state_id', ax=ax[0][0])\nd2 = sns.countplot(sales['store_id'], order= sales['store_id'].value_counts().index, label='store_id', ax=ax[0][1])\nd3 = sns.countplot(sales['cat_id'], order= sales['cat_id'].value_counts().index, label='cat_id', ax=ax[1][0])\nd4 = sns.countplot(sales['dept_id'], order= sales['dept_id'].value_counts().index, label='dept_id', ax=ax[1][1])\nd1.set_xticklabels(d1.get_xticklabels(), rotation=45)\nd1.set_title('state_id')\nd2.set_xticklabels(d2.get_xticklabels(), rotation=45)\nd2.set_title('store_id')\nd3.set_xticklabels(d3.get_xticklabels(), rotation=45)\nd3.set_title('category_id')\nd4.set_xticklabels(d4.get_xticklabels(), rotation=45)\nd4.set_title('dept_id')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)\nsell_dist = sns.distplot(selling_prices['sell_price'], hist=True, color='skyblue', label='sell_dist', bins=100, ax=ax[0])\nfull_sell_dist = sns.distplot(selling_prices['sell_price'], hist=True, color='skyblue', label='sell_dist', bins=200, ax=ax[1])\nsell_dist.set(title='Distribution with cutoff', ylabel='Density', xlabel = 'Price',  xlim=(0, 25))\nfull_sell_dist.set(title='Distribution without cutoff', ylabel='Density', xlabel = 'Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,5), sharex=False)#\n#sell_dist = sns.distplot(np.log(selling_prices['sell_price']), hist=True, color='skyblue', label='sell_dist', bins=100, ax=ax[0])\nfull_sell_dist = sns.distplot(np.log(selling_prices['sell_price']), hist=True, color='skyblue', label='sell_dist', bins=100, ax=ax)\nfull_sell_dist.set(title='Log Price Distribution', ylabel='Density', xlabel = 'Log Price', xlim=(-1.5,3.5))\n#sell_dist.set(title='Log distribution with cutoff With Gaussian', ylabel='Probability', xlabel = 'Price',  xlim=(-5, 5))\nx = np.random.normal(1.3, 0.6, size=100)\nsns.kdeplot(x, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)\nsell_dist = sns.distplot(selling_prices['sell_price'], hist=True, color='skyblue', label='sell_dist', bins=100, ax=ax[0])\nfull_sell_dist = sns.distplot(np.log(selling_prices['sell_price']), hist=True, color='skyblue', label='sell_dist', bins=100, ax=ax[1])\nfull_sell_dist.set(title='Log Price Distribution', ylabel='Density', xlabel = 'Log Price', xlim=(-1.5,3.5))\nsell_dist.set(title='Distribution with cutoff', ylabel='Density', xlabel = 'Price',  xlim=(0, 25))\nx = np.random.normal(1.3, 0.6, size=100)\nsns.kdeplot(x, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adapted from https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration\nd_cols = [c for c in sales.columns if r'd_' in c] # sales data columns\npast_sales = sales.set_index('id')[d_cols].T.merge(calendar.set_index('d')['date'], left_index=True, right_index=True).set_index('date')\n\nfor i in sales['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col].sum(axis=1).plot(figsize=(15, 5), title='Total Sales by Item Type')\nplt.legend(sales['cat_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in sales['dept_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col].sum(axis=1).plot(figsize=(15, 5), title='Total Sales by department')\nplt.legend(sales['dept_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in sales['state_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col].sum(axis=1).plot(figsize=(15, 5), title='Total Sales by State')\nplt.legend(sales['state_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_col = [c for c in past_sales.columns]\npast_sales[items_col].sum(axis=1).plot(figsize=(15, 5), title='Total Sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = past_sales[items_col].sum(axis=1)\nps[ps < 100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_file = open('encoders', 'wb')\n'''\n- The columns \"Date\" and \"Weekday\" are dropped as they contain redundant information.\n- Normally, the column \"d\" is like \"d_1,d_2,...\". Make it \"1,2,..\" and the type integer\n- If there is no event (I think), there is NA. We will replace them with \"NoEvent\" string. Originally, it was replaced with\n  \"missing\", but I don't think it makes sense as I don't think there is missing information, I think they just left \n  the days without any event as NA.\n- We enumerate most of the columns:\n    - We do not enumerate \"d\" and \"wm_yr_weak\" because we will use these columns for joins.\n    - Why do we enumerate month and day? I think it is because they start from 1, not 0.\n    - Originally, the binary columns \"snap_X\" were also enumerated. I don't think it is necessary. The only neccessary step\n      was to convert their type from int64 to int as it uses less space; but reduce_mem_usage will take care of that.\n- I would suggest saving the OrdinalEncoder in case we need to reverse the transformations\n'''\ndef prep_calendar(df,encoder_file):\n    df = df.drop([\"date\", \"weekday\"], axis=1)  \n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"NoEvent\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\",\"snap_CA\",\"snap_TX\",\"snap_WI\"}) \n    oe = OrdinalEncoder(dtype=\"int\")\n    df[cols] = oe.fit_transform(df[cols])\n    pickle.dump(oe,encoder_file)\n    df = reduce_mem_usage(df)\n    return df\n\n'''\nGot from: [1]\nOriginally, there were features added in this part. I excluded them until we decide whether to use those or not.\n'''\ndef prep_selling_prices(df):\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar, encoder_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n- We drop the first \"drop_d\" days. Originally, this is set to 1000. When it is set to this value,\n  the shape we get 29,544,810 rows. When we don't set it, we get 60,034,810 rows. I think for now \n  we can keep this functionality, as it may be useful if we would like to discard some of the days.\n- In some id's, we have \"_validation\". Those are deleted.\n- reindex: Conform DataFrame to new index with optional filling logic (obtained from pandas doc). \n  We add days 1914+2*28 to prepare data from submission\n- We have to melt the sales dataframe since days are contained as columns.\n- assign: Returns a new object with all original columns in addition to new ones. Existing columns \n  that are re-assigned will be overwritten (obtained from pandas doc). Again, we make the values \n  \"d_1, d-2,...\" to \"1,2,...\"\n'''\n#We have to melt sales for sure because the days are columns, which is not desirable.\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ndel calendar\ngc.collect()\ntime.sleep(5)\nsales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\ndel selling_prices\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\ntime.sleep(5)\nsales.head()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(6,5), sharex=False)#\ndemand_dist_cutoff = sns.distplot(sales['demand'], hist=True, color='skyblue', label='demand_dist', bins = 900, ax=ax)\ndemand_dist_cutoff.set(title='Cutoff Demand Distribution', ylabel='Density', xlabel = 'Demand', xlim=(0,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)#\ndemand_dist = sns.distplot(sales['demand'], hist=True, color='skyblue', label='demand_dist', ax=ax[0])\ndemand_dist_cutoff = sns.distplot(sales['demand'], hist=True, color='skyblue', label='demand_dist', bins = 900, ax=ax[1])\ndemand_dist.set(title='Demand Distribution', ylabel='Density', xlabel = 'Demand')\ndemand_dist_cutoff.set(title='Cutoff Demand Distribution', ylabel='Density', xlabel = 'Demand', xlim=(0,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tweedie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tweedie\nfig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)\ntvs = tweedie.tweedie(mu=1, p=1.5, phi=2).rvs(1000)\ntweedie = sns.distplot(tvs, color=\"navy\", ax=ax[0])\ndemand_dist_cutoff = sns.distplot(sales['demand'], hist=True, color='navy', label='demand_dist', bins = 900, ax=ax[1])\ndemand_dist_cutoff.set(title='Cutoff Demand Distribution', ylabel='Density', xlabel = 'Demand', xlim=(0,15))\ntweedie.set(title='Tweedie Distribution', ylabel='y', xlabel = 'x')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tweedie\nfig, ax = plt.subplots(1,2, figsize=(12,5), sharex=False)\ntvs = tweedie.tweedie(mu=1, p=1.5, phi=2).rvs(10000)\ntweedie = sns.distplot(tvs, color=\"skyblue\", ax=ax[1])\ndemand_dist_cutoff = sns.distplot(sales['demand'], hist=True, color='skyblue', label='demand_dist', bins = 900, ax=ax[0])\ndemand_dist_cutoff.set(title='Cutoff Demand Distribution', ylabel='Density', xlabel = 'Demand', xlim=(0,15))\ntweedie.set(title='Tweedie Distribution', ylabel='Density', xlabel = 'Count', xlim=(0,15))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}