{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.kaggle.com/mayer79/m5-forecast-keras-with-categorical-embeddings-v2 \n",
    "\n",
    "[2] https://www.kaggle.com/ragnar123/very-fst-model\n",
    "\n",
    "[3] https://www.kaggle.com/mayer79/m5-forecast-poisson-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import time\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_poisson_deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Memory Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function on dataframes to reduce the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file to store encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file = open('encoders', 'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "path = \"../input/m5-forecasting-accuracy\"\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\n",
    "sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- The columns \"Date\" and \"Weekday\" are dropped as they contain redundant information.\n",
    "- Normally, the column \"d\" is like \"d_1,d_2,...\". Make it \"1,2,..\" and the type integer\n",
    "- If there is no event (I think), there is NA. We will replace them with \"NoEvent\" string. Originally, it was replaced with\n",
    "  \"missing\", but I don't think it makes sense as I don't think there is missing information, I think they just left \n",
    "  the days without any event as NA.\n",
    "- We enumerate most of the columns:\n",
    "    - We do not enumerate \"d\" and \"wm_yr_weak\" because we will use these columns for joins.\n",
    "    - Why do we enumerate month and day? I think it is because they start from 1, not 0.\n",
    "    - Originally, the binary columns \"snap_X\" were also enumerated. I don't think it is necessary. The only neccessary step\n",
    "      was to convert their type from int64 to int as it uses less space; but reduce_mem_usage will take care of that.\n",
    "- I would suggest saving the OrdinalEncoder in case we need to reverse the transformations\n",
    "'''\n",
    "def prep_calendar(df,encoder_file):\n",
    "    df = df.drop([\"date\", \"weekday\",\"event_name_1\", \"event_name_2\", \"event_type_2\",\"event_type_1\",\"snap_CA\",\"snap_TX\",\"snap_WI\"], axis=1)  \n",
    "    df = df.assign(d = df.d.str[2:].astype(int))\n",
    "    df = df.fillna(\"NoEvent\")\n",
    "    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"}) \n",
    "    oe = OrdinalEncoder(dtype=\"int\")\n",
    "    df[cols] = oe.fit_transform(df[cols])\n",
    "    pickle.dump(oe,encoder_file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "calendar = prep_calendar(calendar,encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "Originally, there were features added in this part. I excluded them until we decide whether to use those or not.\n",
    "'''\n",
    "def prep_selling_prices(df):\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "selling_prices = prep_selling_prices(selling_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- We drop the first \"drop_d\" days. Originally, this is set to 1000. When it is set to this value,\n",
    "  the shape we get 29,544,810 rows. When we don't set it, we get 60,034,810 rows. I think for now \n",
    "  we can keep this functionality, as it may be useful if we would like to discard some of the days.\n",
    "- In some id's, we have \"_validation\". Those are deleted.\n",
    "- reindex: Conform DataFrame to new index with optional filling logic (obtained from pandas doc). \n",
    "  We add days 1914+2*28 to prepare data from submission\n",
    "- We have to melt the sales dataframe since days are contained as columns.\n",
    "- assign: Returns a new object with all original columns in addition to new ones. Existing columns \n",
    "  that are re-assigned will be overwritten (obtained from pandas doc). Again, we make the values \n",
    "  \"d_1, d-2,...\" to \"1,2,...\"\n",
    "'''\n",
    "#We have to melt sales for sure because the days are columns, which is not desirable.\n",
    "def reshape_sales(df, drop_d = None):\n",
    "    if drop_d is not None:\n",
    "        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n",
    "    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n",
    "    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n",
    "    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                 var_name='d', value_name='demand')\n",
    "    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n",
    "    return df\n",
    "\n",
    "sales = reshape_sales(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- Merge all the dataframes and delete the unnecessary ones\n",
    "- time.sleep() added to make sure garbage collector finishes its job before the next merge\n",
    "'''\n",
    "sales = sales.merge(calendar, how=\"left\", on=\"d\")\n",
    "del calendar\n",
    "gc.collect()\n",
    "time.sleep(5)\n",
    "sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "del selling_prices\n",
    "sales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\n",
    "gc.collect()\n",
    "time.sleep(5)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- We will also save the encoders in the pickle file.\n",
    "- The loop is slightly changed\n",
    "'''\n",
    "\n",
    "cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
    "\n",
    "# In loop to minimize memory use\n",
    "for col in cat_id_cols:\n",
    "    oe = OrdinalEncoder(dtype=\"int\")\n",
    "    sales[col] = oe.fit_transform(sales[[col]])\n",
    "    pickle.dump(oe,encoder_file)    \n",
    "sales = reduce_mem_usage(sales)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing NaN of demand with 0 \n",
    "\n",
    "sales[\"demand\"] = sales[\"demand\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do train,test, validation split; for some time we have to store all data twice to be able to divide the dataset. And every feature we add increases the size of the dataset drastically. Hence, if you have memory issues, I would suggest doing this part after splitting, even if it means we have to execute each line 3 times (for train,test,valid). And we have to be careful for the first few rows because these features sometimes depend on previous datapoints; which may be found in train set for first few rows of validation, for example.\n",
    "\n",
    "KNN also may require some extra data storage. Moreover, in that case, this part can be even after KNN. This way, you can also use imputed selling prices to calculate your new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "- These features were originally added in prep_selling_prices function to the \n",
    "  selling_prices dataframe, which does not exist anymore. But we can use the same\n",
    "  code to add these to sales dataframe as well as the columns are the same.\n",
    "- New feature=\"sell_price_rel_diff\"\n",
    "  pct_change(): Computes the percentage change from the immediately previous row by default. (Obtained from pandas doc.)\n",
    "  The two lines below adds the percentage of change of each item that is sold in the stores. Of course, for the\n",
    "  first datapoint, there is no previous, so this code produces an NA. Since there are 3049x10=30490 different (item,store)\n",
    "  pairs, this new column has 30490 NAs.\n",
    "- New feature=\"sell_price_roll_sd7\"\n",
    "  Rolling standard deviation: Moving Standard Deviation is a statistical measurement of market volatility (Google). We check the\n",
    "  past 7 days.\n",
    "- New feature=\"sell_price_cumrel\"\n",
    "  I think this is cumulative related frequency. I am not sure and I did not understand it clearly. I think\n",
    "  it is some kind of normalization, because we subtract the minimum and divide by max-min+1.\n",
    "- It runs without problems, but the RAM gets almost filled up so it gives you a heart attack.\n",
    "- No need to call reduce_mem_usage() after as I tried and it did not save any additional space.\n",
    "'''\n",
    "\n",
    "#gr = sales.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n",
    "#sales[\"sell_price_rel_diff\"] = gr.pct_change()\n",
    "\n",
    "#del gr\n",
    "#gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy;\n",
    "from scipy import stats\n",
    "'''\n",
    "Got from: [1]\n",
    "Appearantly, [1] derived the features from [2].\n",
    "\n",
    "- The features include rolling means and standard deviations for different number of days.\n",
    "  I think this is also a measure of market volatiliy.\n",
    "- After this, original notebook deletes rows producing NAs. 'rolling_mean_t180' produces the most NAs as I\n",
    "  believe the value of first 180 days will be NA because to compute this we need 180 days prior. I do not \n",
    "  know whether adding these features is so important to delete that much data. Hence, I will keep it for now.\n",
    "  I think time ranges can also be changed.\n",
    "'''\n",
    "\n",
    "gr = sales.groupby([\"id\"])[\"demand\"]\n",
    "\n",
    "# fourier \n",
    "fourier = gr.transform(lambda x: np.fft.ifft(x))\n",
    "sales['ifourier_lag_t14'],sales['ifourier_lag_t28'] = fourier,fourier\n",
    "print(sales.head(10))\n",
    "sales['ifourier_lag_t14'] = sales['ifourier_lag_t14'].transform(lambda x: x.shift(14))\n",
    "sales['ifourier_lag_t28'] = sales['ifourier_lag_t28'].transform(lambda x: x.shift(28))\n",
    "\n",
    "fourier = gr.transform(lambda x: scipy.fft.idct(x , type=2, norm='ortho'))\n",
    "sales['idfourier_lag_t14'], sales['idfourier_lag_t28'] = fourier, fourier\n",
    "print(sales.head(10))\n",
    "sales['idfourier_lag_t14'] = sales['idfourier_lag_t14'].transform(lambda x: x.shift(14)) \n",
    "sales['idfourier_lag_t28'] = sales[\"idfourier_lag_t28\"].transform(lambda x: x.shift(28))\n",
    "\n",
    "\n",
    "\n",
    "del gr, fourier\n",
    "sales = reduce_mem_usage(sales)\n",
    "gc.collect()\n",
    "\n",
    "# commented this just so see impact of fourier \n",
    "\n",
    "'''sales['rolling_mean_t7'] = gr.transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "sales['rolling_mean_t30'] = gr.transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "sales['rolling_mean_t60'] = gr.transform(lambda x: x.shift(28).rolling(60).mean())\n",
    "sales['rolling_mean_t180'] = gr.transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "#sales['rolling_std_t7'] = gr.transform(lambda x: x.shift(28).rolling(7).std())\n",
    "sales['rolling_std_t30'] = gr.transform(lambda x: x.shift(28).rolling(30).std())\n",
    "'''\n",
    "\n",
    "# lora's custom features\n",
    "'''\n",
    "sales['price_max'] = gr.transform('max')\n",
    "sales['price_min'] = gr.transform('min')\n",
    "sales['price_std'] = gr.transform('std')\n",
    "sales['price_mean'] =gr.transform('mean')\n",
    "sales['price_norm'] = sales['sell_price']/sales['price_max']\n",
    "sales['price_momentum'] =   sales['sell_price']/gr.transform(lambda x: x.shift(1))\n",
    "sales['price_momentum_m'] = sales['sell_price']/gr.transform('mean')\n",
    "sales['price_momentum_y'] = sales['sell_price']/gr.transform('mean')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del  fourier\n",
    "sales = reduce_mem_usage(sales)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming convention:\n",
    "* test, train, valid -> Use these to create your X_{train,valid,test}  and y_{train,valid}.\n",
    "* Training data: X_train, y_train\n",
    "* Validation data: X_valid, y_valid\n",
    "* Testing data (Kaggle has the labels): X_test\n",
    "\n",
    "X_{train,valid,test} should be preprocessed according to your model.\n",
    "\n",
    "Since we will use test dataframe for submission and the submission is obtained from another person, make sure the order of X_test and test is the same.\n",
    "\n",
    "This process increases memory usage from 6GB to 7.5GB and reduce_mem_usage() does not improve the situation. One idea can be to keep the flags generated but produce end datasets at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "test = sales[sales.d >= 1914]\n",
    "test = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n",
    "                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\n",
    "gc.collect()\n",
    "\n",
    "# One month of validation data\n",
    "flag_valid = (sales.d < 1914) & (sales.d >= 1914 - 28)\n",
    "valid = sales[flag_valid]\n",
    "flag_train = sales.d < 1914 - 28\n",
    "train = sales[flag_train]\n",
    "\n",
    "\n",
    "del sales, flag_valid,flag_train\n",
    "gc.collect()\n",
    "time.sleep(5)\n",
    "\n",
    "test.reset_index(drop=True)\n",
    "valid.reset_index(drop=True)\n",
    "train.reset_index(drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation says: If (selling_price) not available, this means that the product was not sold during the examined week. \n",
    "\n",
    "When we are not using recurrent structures, I think it is the\n",
    "best if we just remove those rows. However, for recurrent structures, such as LSTM and GRU, it would be better if\n",
    "the sequence was not broken. For those models, we can impute the missing labels with KNN. So there will be two\n",
    "alternatives. Set impute_labels=True to impute, False to drop depending on your application.\n",
    "\n",
    "Note that we have to do the imputation after we split the data to prevent leaking information.\n",
    "\n",
    "Don't use the demand column not to leak data from the label.\n",
    "\n",
    "I could not use KNN, SVR etc due to memory limitations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NAs:\\n\",train.isna().sum())\n",
    "impute_labels = True\n",
    "\n",
    "if not impute_labels:\n",
    "    train.dropna(inplace=True)\n",
    "    valid.dropna(inplace=True)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    to_be_inputed = [\"ifourier_lag_t14\",\"ifourier_lag_t28\",\n",
    "                    'idfourier_lag_t14','idfourier_lag_t28']\n",
    "    for col in to_be_inputed:\n",
    "        mean_train = train[col].dropna().astype(float).mean(skipna=True)\n",
    "        print(mean_train)\n",
    "        train[col] = train[col].fillna(mean_train)\n",
    "        valid[col] = valid[col].fillna(mean_train)\n",
    "        test[col] = test[col].fillna(mean_train)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to normalize data for the models that are not tree based. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(set(train.columns) - {'id', 'd', 'demand'})\n",
    "train_part = train[train['year'] > 2]\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close encoder file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "** Enter description of the model here **\n",
    "\n",
    "** Name your model \"model\" **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(train_part.drop(['id', 'demand'], axis=1), label = train_part['demand'],\n",
    "                        categorical_feature=['item_id','dept_id','cat_id','store_id','state_id','wday',\n",
    "                                             'month','year'])\n",
    "del train_part\n",
    "valid_set = lgb.Dataset(valid.drop(['id', 'demand'],axis=1), label = valid['demand'],\n",
    "                        categorical_feature=['item_id','dept_id','cat_id','store_id','state_id','wday',\n",
    "                                             'month','year'])\n",
    "del valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from [3]\n",
    "def fit_model(train, valid):\n",
    "    \"\"\" Fit LightGBM model \"\"\"\n",
    "     \n",
    "    params = {\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'poisson',\n",
    "        'seed': 200,\n",
    "        'force_row_wise' : True,\n",
    "        'learning_rate' : 0.075,\n",
    "        #'lambda': 0.1,\n",
    "        'num_leaves': 128,\n",
    "        'sub_row' : 0.7,\n",
    "        'bagging_freq' : 1,\n",
    "        'colsample_bytree': 0.77,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "        \"metric\": [\"rmse\"],\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 1200,\n",
    "    \"min_data_in_leaf\": 100\n",
    "    }\n",
    "\n",
    "    fit = lgb.train(params, \n",
    "                    train, \n",
    "                    num_boost_round = 2000, \n",
    "                    valid_sets = [valid], \n",
    "                    early_stopping_rounds = 200,\n",
    "                    verbose_eval = 100)\n",
    "    \n",
    "    lgb.plot_importance(fit, importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10));\n",
    "    \n",
    "    return fit\n",
    "\n",
    "fit = fit_model(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got from: [1]\n",
    "'''\n",
    "\n",
    "pred = fit.predict(test.drop(['id','F', 'demand'],axis=1))\n",
    "test[\"demand\"] = pred.clip(0)\n",
    "submission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\n",
    "submission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
